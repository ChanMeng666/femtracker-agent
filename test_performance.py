#!/usr/bin/env python3
"""
femtracker-agent æ€§èƒ½å‹åŠ›æµ‹è¯•è„šæœ¬
æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘å’Œå¤§æ•°æ®é‡ä¸‹çš„è¡¨ç°
"""

import asyncio
import aiohttp
import time
import json
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PerformanceTest:
    def __init__(self, base_url: str = "http://localhost:2024", 
                 frontend_url: str = "http://localhost:3000"):
        self.base_url = base_url
        self.frontend_url = frontend_url
        self.results = {}
    
    async def single_request_test(self, session: aiohttp.ClientSession, 
                                url: str, data: Dict = None) -> Dict[str, Any]:
        """å•æ¬¡è¯·æ±‚æµ‹è¯•"""
        start_time = time.time()
        try:
            if data:
                async with session.post(url, json=data, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    content = await response.text()
                    end_time = time.time()
                    return {
                        "success": True,
                        "status_code": response.status,
                        "response_time": end_time - start_time,
                        "content_length": len(content)
                    }
            else:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    content = await response.text()
                    end_time = time.time()
                    return {
                        "success": True,
                        "status_code": response.status,
                        "response_time": end_time - start_time,
                        "content_length": len(content)
                    }
        except Exception as e:
            end_time = time.time()
            return {
                "success": False,
                "error": str(e),
                "response_time": end_time - start_time
            }
    
    async def concurrent_load_test(self, url: str, concurrent_users: int = 10, 
                                 requests_per_user: int = 5, data: Dict = None) -> Dict[str, Any]:
        """å¹¶å‘è´Ÿè½½æµ‹è¯•"""
        logger.info(f"ğŸ”¥ å¼€å§‹å¹¶å‘æµ‹è¯•: {concurrent_users} ç”¨æˆ·, æ¯ç”¨æˆ· {requests_per_user} è¯·æ±‚")
        
        start_time = time.time()
        
        async with aiohttp.ClientSession() as session:
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = []
            for user in range(concurrent_users):
                for request in range(requests_per_user):
                    task = self.single_request_test(session, url, data)
                    tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # åˆ†æç»“æœ
        successful_requests = [r for r in results if isinstance(r, dict) and r.get("success", False)]
        failed_requests = [r for r in results if not (isinstance(r, dict) and r.get("success", False))]
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 1 else avg_response_time
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = 0
        
        success_rate = len(successful_requests) / len(tasks) if tasks else 0
        requests_per_second = len(tasks) / total_duration if total_duration > 0 else 0
        
        return {
            "total_requests": len(tasks),
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "success_rate": success_rate,
            "total_duration": total_duration,
            "requests_per_second": requests_per_second,
            "avg_response_time": avg_response_time,
            "min_response_time": min_response_time,
            "max_response_time": max_response_time,
            "p95_response_time": p95_response_time
        }
    
    async def test_frontend_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å‰ç«¯é¡µé¢æ€§èƒ½"""
        logger.info("ğŸŒ æµ‹è¯•å‰ç«¯é¡µé¢æ€§èƒ½...")
        
        pages = [
            "/dashboard",
            "/cycle-tracker", 
            "/symptom-mood",
            "/fertility",
            "/nutrition",
            "/exercise",
            "/lifestyle",
            "/insights"
        ]
        
        page_results = {}
        
        for page in pages:
            url = f"{self.frontend_url}{page}"
            logger.info(f"æµ‹è¯•é¡µé¢: {page}")
            
            result = await self.concurrent_load_test(url, concurrent_users=5, requests_per_user=3)
            page_results[page] = result
            
            logger.info(f"  æˆåŠŸç‡: {result['success_rate']*100:.1f}%")
            logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']*1000:.0f}ms")
            logger.info(f"  QPS: {result['requests_per_second']:.1f}")
        
        # è®¡ç®—æ€»ä½“å‰ç«¯æ€§èƒ½
        all_success_rates = [r['success_rate'] for r in page_results.values()]
        all_response_times = [r['avg_response_time'] for r in page_results.values()]
        
        overall_frontend = {
            "avg_success_rate": statistics.mean(all_success_rates),
            "avg_response_time": statistics.mean(all_response_times),
            "page_results": page_results
        }
        
        self.results["frontend_performance"] = overall_frontend
        return overall_frontend
    
    async def test_api_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•APIæ€§èƒ½"""
        logger.info("ğŸ”§ æµ‹è¯•APIæ€§èƒ½...")
        
        # æ¨¡æ‹ŸAPIè¯·æ±‚
        api_url = f"{self.frontend_url}/api/copilotkit"
        test_data = {
            "message": "è®°å½•ä»Šå¤©çš„æœˆç»æƒ…å†µ",
            "agent": "cycle_tracker"
        }
        
        result = await self.concurrent_load_test(api_url, concurrent_users=8, requests_per_user=3, data=test_data)
        
        logger.info(f"APIæ€§èƒ½æµ‹è¯•ç»“æœ:")
        logger.info(f"  æˆåŠŸç‡: {result['success_rate']*100:.1f}%")
        logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']*1000:.0f}ms")
        logger.info(f"  QPS: {result['requests_per_second']:.1f}")
        logger.info(f"  P95å“åº”æ—¶é—´: {result['p95_response_time']*1000:.0f}ms")
        
        self.results["api_performance"] = result
        return result
    
    async def test_data_processing_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®å¤„ç†æ€§èƒ½"""
        logger.info("ğŸ“Š æµ‹è¯•æ•°æ®å¤„ç†æ€§èƒ½...")
        
        # æ¨¡æ‹Ÿå¤§é‡å¥åº·æ•°æ®å¤„ç†
        large_dataset = {
            "user_id": "performance_test_user",
            "data_points": [
                {
                    "date": f"2024-{month:02d}-{day:02d}",
                    "cycle_day": (month * 30 + day) % 28 + 1,
                    "symptoms": ["cramping", "bloating"],
                    "mood": "neutral",
                    "exercise_minutes": 30,
                    "nutrition_score": 75
                }
                for month in range(1, 7)  # 6ä¸ªæœˆæ•°æ®
                for day in range(1, 31)   # æ¯æœˆ30å¤©
            ]
        }
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†ç®—æ³•
        processed_data = await self._simulate_data_processing(large_dataset)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        data_size_mb = len(json.dumps(large_dataset).encode('utf-8')) / (1024 * 1024)
        processing_speed = data_size_mb / processing_time if processing_time > 0 else 0
        
        result = {
            "data_points": len(large_dataset["data_points"]),
            "data_size_mb": data_size_mb,
            "processing_time": processing_time,
            "processing_speed_mbps": processing_speed,
            "processed_successfully": processed_data.get("success", False)
        }
        
        logger.info(f"æ•°æ®å¤„ç†æ€§èƒ½:")
        logger.info(f"  æ•°æ®ç‚¹æ•°é‡: {result['data_points']}")
        logger.info(f"  æ•°æ®å¤§å°: {result['data_size_mb']:.2f} MB")
        logger.info(f"  å¤„ç†æ—¶é—´: {result['processing_time']:.2f} ç§’")
        logger.info(f"  å¤„ç†é€Ÿåº¦: {result['processing_speed_mbps']:.2f} MB/s")
        
        self.results["data_processing"] = result
        return result
    
    async def _simulate_data_processing(self, data: Dict) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæ•°æ®å¤„ç†é€»è¾‘"""
        try:
            # æ¨¡æ‹Ÿå¤æ‚çš„æ•°æ®åˆ†æè®¡ç®—
            data_points = data["data_points"]
            
            # è®¡ç®—å¹³å‡å€¼
            avg_cycle_day = sum(point["cycle_day"] for point in data_points) / len(data_points)
            
            # è®¡ç®—è¶‹åŠ¿
            exercise_trend = sum(point["exercise_minutes"] for point in data_points) / len(data_points)
            
            # æ¨¡æ‹ŸAIåˆ†æå»¶è¿Ÿ
            await asyncio.sleep(0.1)
            
            return {
                "success": True,
                "avg_cycle_day": avg_cycle_day,
                "exercise_trend": exercise_trend,
                "insights": ["æ•°æ®å¤„ç†å®Œæˆ", "è¶‹åŠ¿åˆ†æå®Œæˆ"]
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        logger.info("ğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨...")
        
        import psutil
        import os
        
        # è·å–å½“å‰è¿›ç¨‹
        process = psutil.Process(os.getpid())
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # åˆ›å»ºå¤§é‡æ•°æ®æ¥æµ‹è¯•å†…å­˜ä½¿ç”¨
        large_data_sets = []
        for i in range(100):
            large_data_sets.append({
                "user_data": list(range(1000)),
                "health_records": [{"day": j, "score": j % 100} for j in range(500)]
            })
        
        # è®°å½•å³°å€¼å†…å­˜
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ¸…ç†æ•°æ®
        large_data_sets.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # è®°å½•æ¸…ç†åå†…å­˜
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        result = {
            "initial_memory_mb": initial_memory,
            "peak_memory_mb": peak_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": peak_memory - initial_memory,
            "memory_leak_mb": final_memory - initial_memory
        }
        
        logger.info(f"å†…å­˜ä½¿ç”¨æµ‹è¯•:")
        logger.info(f"  åˆå§‹å†…å­˜: {result['initial_memory_mb']:.1f} MB")
        logger.info(f"  å³°å€¼å†…å­˜: {result['peak_memory_mb']:.1f} MB")
        logger.info(f"  æœ€ç»ˆå†…å­˜: {result['final_memory_mb']:.1f} MB")
        logger.info(f"  å†…å­˜å¢é•¿: {result['memory_increase_mb']:.1f} MB")
        logger.info(f"  å¯èƒ½æ³„æ¼: {result['memory_leak_mb']:.1f} MB")
        
        self.results["memory_usage"] = result
        return result
    
    async def run_all_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹ Phase 4 æ€§èƒ½å‹åŠ›æµ‹è¯•")
        logger.info("=" * 60)
        
        start_time = time.time()
        overall_results = {
            "start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tests": {}
        }
        
        # æ‰§è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
        tests = [
            ("Frontend Performance", self.test_frontend_performance),
            ("API Performance", self.test_api_performance),
            ("Data Processing Performance", self.test_data_processing_performance),
            ("Memory Usage", self.test_memory_usage)
        ]
        
        for test_name, test_func in tests:
            logger.info(f"\n{'='*20} {test_name} {'='*20}")
            try:
                result = await test_func()
                overall_results["tests"][test_name] = {
                    "success": True,
                    "result": result
                }
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {test_name} - {str(e)}")
                overall_results["tests"][test_name] = {
                    "success": False,
                    "error": str(e)
                }
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        overall_results.update({
            "end_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_duration": total_duration,
            "summary": self._generate_performance_summary()
        })
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  æ€»æµ‹è¯•æ—¶é•¿: {total_duration:.2f} ç§’")
        
        # è¾“å‡ºæ€§èƒ½è¯„ä¼°
        self._print_performance_assessment(overall_results)
        
        return overall_results
    
    def _generate_performance_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“"""
        summary = {}
        
        if "frontend_performance" in self.results:
            fp = self.results["frontend_performance"]
            summary["frontend"] = {
                "avg_success_rate": fp["avg_success_rate"],
                "avg_response_time_ms": fp["avg_response_time"] * 1000,
                "status": "è‰¯å¥½" if fp["avg_success_rate"] > 0.9 and fp["avg_response_time"] < 2.0 else "éœ€è¦ä¼˜åŒ–"
            }
        
        if "api_performance" in self.results:
            ap = self.results["api_performance"]
            summary["api"] = {
                "success_rate": ap["success_rate"],
                "avg_response_time_ms": ap["avg_response_time"] * 1000,
                "qps": ap["requests_per_second"],
                "status": "è‰¯å¥½" if ap["success_rate"] > 0.8 and ap["avg_response_time"] < 1.0 else "éœ€è¦ä¼˜åŒ–"
            }
        
        if "data_processing" in self.results:
            dp = self.results["data_processing"]
            summary["data_processing"] = {
                "processing_speed_mbps": dp["processing_speed_mbps"],
                "processing_time": dp["processing_time"],
                "status": "è‰¯å¥½" if dp["processing_speed_mbps"] > 1.0 else "éœ€è¦ä¼˜åŒ–"
            }
        
        if "memory_usage" in self.results:
            mu = self.results["memory_usage"]
            summary["memory"] = {
                "peak_memory_mb": mu["peak_memory_mb"],
                "memory_leak_mb": mu["memory_leak_mb"],
                "status": "è‰¯å¥½" if mu["memory_leak_mb"] < 10 else "éœ€è¦å…³æ³¨"
            }
        
        return summary
    
    def _print_performance_assessment(self, results: Dict[str, Any]):
        """æ‰“å°æ€§èƒ½è¯„ä¼°ç»“æœ"""
        summary = results.get("summary", {})
        
        logger.info("ğŸ¯ æ€§èƒ½è¯„ä¼°ç»“æœ:")
        
        for category, metrics in summary.items():
            status = metrics.get("status", "æœªçŸ¥")
            status_icon = "âœ…" if status == "è‰¯å¥½" else "âš ï¸" if status == "éœ€è¦ä¼˜åŒ–" else "âŒ"
            logger.info(f"  {status_icon} {category}: {status}")
        
        # æ€»ä½“æ€§èƒ½è¯„çº§
        good_count = sum(1 for m in summary.values() if m.get("status") == "è‰¯å¥½")
        total_count = len(summary)
        
        if total_count > 0:
            performance_ratio = good_count / total_count
            if performance_ratio >= 0.8:
                overall_status = "ä¼˜ç§€"
                status_icon = "ğŸ‰"
            elif performance_ratio >= 0.6:
                overall_status = "è‰¯å¥½"
                status_icon = "âœ…"
            else:
                overall_status = "éœ€è¦ä¼˜åŒ–"
                status_icon = "âš ï¸"
            
            logger.info(f"\n{status_icon} æ€»ä½“æ€§èƒ½è¯„çº§: {overall_status} ({good_count}/{total_count})")
        

def save_performance_report(results: Dict[str, Any]):
    """ä¿å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    # ä¿å­˜è¯¦ç»†JSONç»“æœ
    with open("performance_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report = f"""
# Phase 4: æ€§èƒ½å‹åŠ›æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- **å¼€å§‹æ—¶é—´**: {results['start_time']}
- **ç»“æŸæ—¶é—´**: {results['end_time']}
- **æ€»æµ‹è¯•æ—¶é•¿**: {results['total_duration']:.2f} ç§’

## æ€§èƒ½æµ‹è¯•ç»“æœ

"""
    
    summary = results.get("summary", {})
    for category, metrics in summary.items():
        status = metrics.get("status", "æœªçŸ¥")
        status_icon = "âœ…" if status == "è‰¯å¥½" else "âš ï¸" if status == "éœ€è¦ä¼˜åŒ–" else "âŒ"
        
        report += f"### {category.replace('_', ' ').title()}\n"
        report += f"**çŠ¶æ€**: {status_icon} {status}\n\n"
        
        for key, value in metrics.items():
            if key != "status":
                if isinstance(value, float):
                    report += f"- {key}: {value:.2f}\n"
                else:
                    report += f"- {key}: {value}\n"
        report += "\n"
    
    report += """
## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å‰ç«¯ä¼˜åŒ–**
   - å®æ–½ä»£ç åˆ†å‰²å’Œæ‡’åŠ è½½
   - ä¼˜åŒ–å›¾ç‰‡å’Œé™æ€èµ„æº
   - ä½¿ç”¨CDNåŠ é€Ÿ

2. **APIä¼˜åŒ–**  
   - å®æ–½ç¼“å­˜ç­–ç•¥
   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
   - ä½¿ç”¨è¿æ¥æ± 

3. **æ•°æ®å¤„ç†ä¼˜åŒ–**
   - å®æ–½å¼‚æ­¥å¤„ç†
   - ä½¿ç”¨æ‰¹å¤„ç†æŠ€æœ¯
   - ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦

4. **å†…å­˜ç®¡ç†**
   - å®šæœŸå†…å­˜æ¸…ç†
   - ä¼˜åŒ–æ•°æ®ç»“æ„
   - ç›‘æ§å†…å­˜æ³„æ¼
"""
    
    with open("performance_test_report.md", "w", encoding="utf-8") as f:
        f.write(report)

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ femtracker-agent æ€§èƒ½å‹åŠ›æµ‹è¯•")
    print("ğŸ“ æµ‹è¯•èŒƒå›´: å‰ç«¯æ€§èƒ½ã€APIæ€§èƒ½ã€æ•°æ®å¤„ç†ã€å†…å­˜ä½¿ç”¨")
    print()
    
    tester = PerformanceTest()
    results = await tester.run_all_performance_tests()
    
    # ä¿å­˜æŠ¥å‘Š
    save_performance_report(results)
    
    print(f"\nğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"  - è¯¦ç»†æ•°æ®: performance_test_results.json")
    print(f"  - æ€»ç»“æŠ¥å‘Š: performance_test_report.md")

if __name__ == "__main__":
    # å®‰è£…å¿…è¦çš„ä¾èµ–
    try:
        import aiohttp
        import psutil
    except ImportError:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œè¯·å®‰è£…: pip install aiohttp psutil")
        exit(1)
    
    asyncio.run(main()) 